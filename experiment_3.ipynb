{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sparseml.pytorch.optim import ScheduledModifierManager\n",
    "from sparseml.pytorch.optim import ScheduledOptimizer\n",
    "from sparseml.pytorch.utils import tensor_sparsity, get_prunable_layers\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/webexpert/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/webexpert/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/webexpert/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/webexpert/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install -q nltk\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20 Newsgroups dataset contains 2752 documents and 3 categories.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load 20 Newsgroups Dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=['rec.autos', 'sci.space', 'talk.politics.misc'])\n",
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "print(f'The 20 Newsgroups dataset contains {len(X)} documents and {len(set(y))} categories.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: kbanaian@bernard.pitzer.claremont.edu (King Banaian)',\n",
       " 'Subject: Re: National Sales Tax, The Movie',\n",
       " 'Lines: 43',\n",
       " 'Organization: Pitzer College',\n",
       " '',\n",
       " 'In article <VEAL.731.734985604@utkvm1.utk.edu> VEAL@utkvm1.utk.edu (David Veal) writes:',\n",
       " '>In article <1993Apr16.164750.21913@alchemy.chem.utoronto.ca> golchowy@',\n",
       " 'alchemy.chem.utoronto.ca (Gerald Olchowy) writes:>',\n",
       " '>>In article <9304151442.AA05233@inet-gw-2.pa.dec.com> blh@uiboise.idbsu.edu (Broward L. Horne) writes:',\n",
       " '>>',\n",
       " \">>Why don't the Republicans get their act together, and say they\",\n",
       " '>>will support a broad-based VAT that would have to be visible',\n",
       " '>>(the VAT in Canada is visible unlike the invisible VATS they',\n",
       " '>>have in Europe)',\n",
       " '>>and suggest a rate sufficient to halve income and corporate',\n",
       " '>>and capital gains tax rates and at a rate sufficient to give',\n",
       " '>>the Clintons enough revenue for their health care reform, ',\n",
       " '>',\n",
       " '>       The Republicans are, in general, fighting any tax increase.',\n",
       " '>There is also worry that a VAT would be far too easy to increase',\n",
       " '>incrementally.',\n",
       " '>',\n",
       " 'I was a graduate student in the early 1980s, and we had a conference on ',\n",
       " 'Reaganomics where Jerry Jordan, then a member of the Council of Economic ',\n",
       " 'Advisors, was a speaker.  I had the pleasure of driving him back to the ',\n",
       " 'airport afterwards, and since taxes were the main topic of discussion I ',\n",
       " 'thought I would ask him about the VAT.  I have favored it for these reasons ',\n",
       " 'you mention, that the income base is too hazy to define, that it taxes ',\n",
       " 'savings and investment, that it is likely to be more visible.  He agreed, ',\n",
       " 'and reported that the CEA at that time was in favor of VAT.  So why not ',\n",
       " 'propose it?  I asked.  He replied that the Reagan White House feared that ',\n",
       " 'the Democrats would introduce VAT *in addition to* the income tax, rather ',\n",
       " 'than in lieu.  Better not to give them any ideas, he said.',\n",
       " '',\n",
       " 'Pretty prescient.',\n",
       " '',\n",
       " \">       (BTW, what is different between Canada's tax and most of\",\n",
       " '>Europe\\'s that makes it \"visible?\")',\n",
       " '>',\n",
       " 'Yes, any Canadian readers, please tell us if the tax is displayed on price ',\n",
       " \"stickers (I'm relatively certain it is not in Europe).\",\n",
       " '',\n",
       " '--King \"Sparky\" Banaian                 |\"It\\'s almost as though young',\n",
       " 'kbanaian@pitzer.claremont.edu           |white guys get up in the',\n",
       " 'Dept. of Economics, Pitzer College      |morning and have a big smile',\n",
       " 'Latest 1993 GDP forecast:  2.4%         |on their face ... because,',\n",
       " '                                        |you know, Homer wrote the',\n",
       " '                                        |_Iliad_.\"  -- D\\'Souza']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1].splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, min_word_frequency=2):\n",
    "    # Initialize stemmer, lemmatizer, and stopwords\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    tokens = [word for word in tokens if not word.isdigit()]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Experiment 1: 150, 8, 8:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2201, 150) (551, 150) (2201,) (551,)\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = [clean_text(i) for i in X]\n",
    "vectorizer = TfidfVectorizer(max_features=150)\n",
    "cleaned_text = vectorizer.fit_transform(cleaned_text).toarray() \n",
    "scaler = StandardScaler()\n",
    "cleaned_text = scaler.fit_transform(cleaned_text)\n",
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_text, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "# Create DataLoader for training and testing\n",
    "trainloader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "testloader = DataLoader(TensorDataset(X_test, y_test), batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu is being used for computation\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device} is being used for computation')\n",
    "\n",
    "# Step 2: Define the Model Architecture\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(150, 8)\n",
    "        self.bn1 = nn.BatchNorm1d(8)\n",
    "        self.fc2 = nn.Linear(8, 8)\n",
    "        self.bn2 = nn.BatchNorm1d(8)\n",
    "        self.fc3 = nn.Linear(8, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: Train the Base Model\n",
    "model = SimpleNN()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Base Model:\n",
      "Epoch 1 - Accuracy: 66.06%\n",
      "Epoch 2 - Accuracy: 80.04%\n",
      "Epoch 3 - Accuracy: 84.03%\n",
      "Epoch 4 - Accuracy: 84.94%\n",
      "Epoch 5 - Accuracy: 85.84%\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train the Base Model\n",
    "# Optimizer (use Adam instead of the non-existent 'ADA' optimizer)\n",
    "base_optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "num_epochs = 5\n",
    "\n",
    "# Step 4: Training Loop\n",
    "train_losses = []\n",
    "print(\"\\nTraining Base Model:\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "        # print(inputs.shape, labels.shape)\n",
    "        base_optimizer.zero_grad()  # Zero gradients before backprop\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "\n",
    "        # Update the weights\n",
    "        base_optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # Record loss every 10 mini-batches\n",
    "            train_losses.append(running_loss / 10)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Step 5: Evaluate the Base Model\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradient tracking during evaluation\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch + 1} - Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 14:42:21 sparseml.pytorch.utils.logger INFO     Logging all SparseML modifier-level logs to sparse_logs/06-12-2024_14.42.21.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Sparsity Level: 60%\n",
      "\n",
      "Training Sparse Model:\n",
      "Epoch 1 - Sparsity: 100.00% - Accuracy: 85.48% - Remaining Weights: 1288/1288\n",
      "Epoch 2 - Sparsity: 58.39% - Accuracy: 86.57% - Remaining Weights: 752/1288\n",
      "Epoch 3 - Sparsity: 46.43% - Accuracy: 85.30% - Remaining Weights: 598/1288\n",
      "Epoch 4 - Sparsity: 41.23% - Accuracy: 84.94% - Remaining Weights: 531/1288\n",
      "Epoch 5 - Sparsity: 39.98% - Accuracy: 85.66% - Remaining Weights: 515/1288\n",
      "\n",
      "Testing Sparsity Level: 70%\n",
      "\n",
      "Training Sparse Model:\n",
      "Epoch 1 - Sparsity: 100.00% - Accuracy: 86.03% - Remaining Weights: 1288/1288\n",
      "Epoch 2 - Sparsity: 52.95% - Accuracy: 85.66% - Remaining Weights: 682/1288\n",
      "Epoch 3 - Sparsity: 38.04% - Accuracy: 85.66% - Remaining Weights: 490/1288\n",
      "Epoch 4 - Sparsity: 31.60% - Accuracy: 86.03% - Remaining Weights: 407/1288\n",
      "Epoch 5 - Sparsity: 29.97% - Accuracy: 86.93% - Remaining Weights: 386/1288\n",
      "\n",
      "Testing Sparsity Level: 75%\n",
      "\n",
      "Training Sparse Model:\n",
      "Epoch 1 - Sparsity: 100.00% - Accuracy: 86.57% - Remaining Weights: 1288/1288\n",
      "Epoch 2 - Sparsity: 50.31% - Accuracy: 84.94% - Remaining Weights: 648/1288\n",
      "Epoch 3 - Sparsity: 33.85% - Accuracy: 84.21% - Remaining Weights: 436/1288\n",
      "Epoch 4 - Sparsity: 26.71% - Accuracy: 87.30% - Remaining Weights: 344/1288\n",
      "Epoch 5 - Sparsity: 25.00% - Accuracy: 85.84% - Remaining Weights: 322/1288\n",
      "\n",
      "Testing Sparsity Level: 80%\n",
      "\n",
      "Training Sparse Model:\n",
      "Epoch 1 - Sparsity: 100.00% - Accuracy: 86.93% - Remaining Weights: 1288/1288\n",
      "Epoch 2 - Sparsity: 47.59% - Accuracy: 84.21% - Remaining Weights: 613/1288\n",
      "Epoch 3 - Sparsity: 29.66% - Accuracy: 84.94% - Remaining Weights: 382/1288\n",
      "Epoch 4 - Sparsity: 21.89% - Accuracy: 84.21% - Remaining Weights: 282/1288\n",
      "Epoch 5 - Sparsity: 20.03% - Accuracy: 86.21% - Remaining Weights: 258/1288\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Test Various Sparsity Levels\n",
    "sparsity_targets = [0.60, 0.70, 0.75, 0.80]\n",
    "for sparsity_target in sparsity_targets:\n",
    "    print(f\"\\nTesting Sparsity Level: {sparsity_target * 100:.0f}%\")\n",
    "\n",
    "    # Reinitialize the Model for Sparsification\n",
    "    sparse_model = SimpleNN()\n",
    "    sparse_model.load_state_dict(model.state_dict())  # Copy weights from the trained base model\n",
    "    sparse_model = sparse_model.to(device)\n",
    "\n",
    "    # Step 6: Apply Sparsification and Train the Sparse Model\n",
    "    # optimizer = torch.optim.SGD(sparse_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "    optimizer = optim.Adam(sparse_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    steps_per_epoch = len(trainloader)\n",
    "\n",
    "    # Create a new sparsification recipe with the target sparsity level\n",
    "    recipe_content = f'''\n",
    "    modifiers:\n",
    "      - !EpochRangeModifier\n",
    "        start_epoch: 0.0\n",
    "        end_epoch: 5.0\n",
    "\n",
    "      - !GlobalMagnitudePruningModifier\n",
    "        params: __ALL_PRUNABLE__\n",
    "        start_epoch: 1.0\n",
    "        end_epoch: 5.0\n",
    "        update_frequency: 0.3\n",
    "        init_sparsity: 0.2\n",
    "        final_sparsity: {sparsity_target}\n",
    "        mask_type: unstructured\n",
    "    '''\n",
    "\n",
    "    with open('temp_recipe.yaml', 'w') as f:\n",
    "        f.write(recipe_content)\n",
    "\n",
    "    manager = ScheduledModifierManager.from_yaml('temp_recipe.yaml')\n",
    "    optimizer = ScheduledOptimizer(optimizer, sparse_model, manager, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "    sparsity_levels = []\n",
    "    accuracies = []\n",
    "    sparse_train_losses = []\n",
    "\n",
    "    print(\"\\nTraining Sparse Model:\")\n",
    "    for epoch in range(num_epochs):\n",
    "        sparse_model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = sparse_model(inputs)\n",
    "            # labels = labels.float().unsqueeze(1)  # Adjust labels shape for binary classification\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:    # Record loss every 10 mini-batches\n",
    "                sparse_train_losses.append(running_loss / 10)\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Step 7: Evaluate Sparsity and Accuracy after Each Epoch\n",
    "        sparse_model.eval()\n",
    "        prunable_layers = get_prunable_layers(sparse_model)\n",
    "        sparsity = 0.0\n",
    "        total_weights = 0\n",
    "        remaining_weights = 0\n",
    "        for (name, layer) in prunable_layers:\n",
    "            layer_sparsity = tensor_sparsity(layer.weight).item()\n",
    "            sparsity += layer_sparsity\n",
    "            total_weights += layer.weight.numel()\n",
    "            remaining_weights += (layer.weight != 0).sum().item()\n",
    "        sparsity = (remaining_weights / total_weights) * 100 if remaining_weights > 0 else 0\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = sparse_model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                # predicted = (outputs > 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        sparsity_levels.append(sparsity)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f'Epoch {epoch + 1} - Sparsity: {sparsity:.2f}% - Accuracy: {accuracy:.2f}% - Remaining Weights: {remaining_weights}/{total_weights}')\n",
    "        # print(f'Epoch {epoch + 1} - Accuracy: {accuracy:.2f}%')\n",
    "        # print(f'Remaining Weights: {remaining_weights}/{total_weights}')\n",
    "\n",
    "    manager.finalize(sparse_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Experiment 2: 300, 8, 4:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2201, 300) (551, 300) (2201,) (551,)\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = [clean_text(i) for i in X]\n",
    "vectorizer = TfidfVectorizer(max_features=300)\n",
    "cleaned_text = vectorizer.fit_transform(cleaned_text).toarray()\n",
    "scaler = StandardScaler()\n",
    "cleaned_text = scaler.fit_transform(cleaned_text)\n",
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_text, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "# Create DataLoader for training and testing\n",
    "trainloader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "testloader = DataLoader(TensorDataset(X_test, y_test), batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu is being used for computation\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device} is being used for computation')\n",
    "\n",
    "# Step 2: Define the Model Architecture\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 8)  # 300 -> 8 neurons\n",
    "        self.bn1 = nn.BatchNorm1d(8)\n",
    "        self.fc2 = nn.Linear(8, 4)    # 8 -> 4 neurons\n",
    "        self.bn2 = nn.BatchNorm1d(4)\n",
    "        self.fc3 = nn.Linear(4, 3)    # 4 -> 3 neurons for classification\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: Train the Base Model\n",
    "model = SimpleNN()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Base Model:\n",
      "Epoch 1 - Accuracy: 60.98%\n",
      "Epoch 2 - Accuracy: 84.39%\n",
      "Epoch 3 - Accuracy: 86.93%\n",
      "Epoch 4 - Accuracy: 91.29%\n",
      "Epoch 5 - Accuracy: 90.02%\n"
     ]
    }
   ],
   "source": [
    "# Optimizer (use Adam instead of the non-existent 'ADA' optimizer)\n",
    "base_optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "num_epochs = 5\n",
    "\n",
    "# Step 4: Training Loop\n",
    "train_losses = []\n",
    "print(\"\\nTraining Base Model:\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "        # print(inputs.shape, labels.shape)\n",
    "        base_optimizer.zero_grad()  # Zero gradients before backprop\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "\n",
    "        # Update the weights\n",
    "        base_optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # Record loss every 10 mini-batches\n",
    "            train_losses.append(running_loss / 10)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Step 5: Evaluate the Base Model\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradient tracking during evaluation\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch + 1} - Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Sparsity Level: 60%\n",
      "\n",
      "Training Sparse Model:\n",
      "Epoch 1 - Sparsity: 100.00% - Accuracy: 90.20% - Remaining Weights: 2444/2444\n",
      "Epoch 2 - Sparsity: 58.39% - Accuracy: 89.47% - Remaining Weights: 1427/2444\n",
      "Epoch 3 - Sparsity: 46.44% - Accuracy: 90.56% - Remaining Weights: 1135/2444\n",
      "Epoch 4 - Sparsity: 41.24% - Accuracy: 89.29% - Remaining Weights: 1008/2444\n",
      "Epoch 5 - Sparsity: 40.02% - Accuracy: 90.38% - Remaining Weights: 978/2444\n",
      "\n",
      "Testing Sparsity Level: 70%\n",
      "\n",
      "Training Sparse Model:\n",
      "Epoch 1 - Sparsity: 100.00% - Accuracy: 91.29% - Remaining Weights: 2444/2444\n",
      "Epoch 2 - Sparsity: 52.99% - Accuracy: 92.20% - Remaining Weights: 1295/2444\n",
      "Epoch 3 - Sparsity: 38.01% - Accuracy: 91.83% - Remaining Weights: 929/2444\n",
      "Epoch 4 - Sparsity: 31.55% - Accuracy: 91.29% - Remaining Weights: 771/2444\n",
      "Epoch 5 - Sparsity: 29.99% - Accuracy: 91.65% - Remaining Weights: 733/2444\n",
      "\n",
      "Testing Sparsity Level: 75%\n",
      "\n",
      "Training Sparse Model:\n",
      "Epoch 1 - Sparsity: 100.00% - Accuracy: 90.38% - Remaining Weights: 2444/2444\n",
      "Epoch 2 - Sparsity: 50.29% - Accuracy: 90.02% - Remaining Weights: 1229/2444\n",
      "Epoch 3 - Sparsity: 33.84% - Accuracy: 89.66% - Remaining Weights: 827/2444\n",
      "Epoch 4 - Sparsity: 26.72% - Accuracy: 90.74% - Remaining Weights: 653/2444\n",
      "Epoch 5 - Sparsity: 25.00% - Accuracy: 91.47% - Remaining Weights: 611/2444\n",
      "\n",
      "Testing Sparsity Level: 80%\n",
      "\n",
      "Training Sparse Model:\n",
      "Epoch 1 - Sparsity: 100.00% - Accuracy: 90.56% - Remaining Weights: 2444/2444\n",
      "Epoch 2 - Sparsity: 47.59% - Accuracy: 90.38% - Remaining Weights: 1163/2444\n",
      "Epoch 3 - Sparsity: 29.62% - Accuracy: 90.74% - Remaining Weights: 724/2444\n",
      "Epoch 4 - Sparsity: 21.89% - Accuracy: 91.83% - Remaining Weights: 535/2444\n",
      "Epoch 5 - Sparsity: 20.01% - Accuracy: 91.65% - Remaining Weights: 489/2444\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Test Various Sparsity Levels\n",
    "sparsity_targets = [0.60, 0.70, 0.75, 0.80]\n",
    "for sparsity_target in sparsity_targets:\n",
    "    print(f\"\\nTesting Sparsity Level: {sparsity_target * 100:.0f}%\")\n",
    "\n",
    "    # Reinitialize the Model for Sparsification\n",
    "    sparse_model = SimpleNN()\n",
    "    sparse_model.load_state_dict(model.state_dict())  # Copy weights from the trained base model\n",
    "    sparse_model = sparse_model.to(device)\n",
    "\n",
    "    # Step 6: Apply Sparsification and Train the Sparse Model\n",
    "    # optimizer = torch.optim.SGD(sparse_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "    optimizer = optim.Adam(sparse_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    steps_per_epoch = len(trainloader)\n",
    "\n",
    "    # Create a new sparsification recipe with the target sparsity level\n",
    "    recipe_content = f'''\n",
    "    modifiers:\n",
    "      - !EpochRangeModifier\n",
    "        start_epoch: 0.0\n",
    "        end_epoch: 5.0\n",
    "\n",
    "      - !GlobalMagnitudePruningModifier\n",
    "        params: __ALL_PRUNABLE__\n",
    "        start_epoch: 1.0\n",
    "        end_epoch: 5.0\n",
    "        update_frequency: 0.3\n",
    "        init_sparsity: 0.2\n",
    "        final_sparsity: {sparsity_target}\n",
    "        mask_type: unstructured\n",
    "    '''\n",
    "\n",
    "    with open('temp_recipe.yaml', 'w') as f:\n",
    "        f.write(recipe_content)\n",
    "\n",
    "    manager = ScheduledModifierManager.from_yaml('temp_recipe.yaml')\n",
    "    optimizer = ScheduledOptimizer(optimizer, sparse_model, manager, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "    sparsity_levels = []\n",
    "    accuracies = []\n",
    "    sparse_train_losses = []\n",
    "\n",
    "    print(\"\\nTraining Sparse Model:\")\n",
    "    for epoch in range(num_epochs):\n",
    "        sparse_model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = sparse_model(inputs)\n",
    "            # labels = labels.float().unsqueeze(1)  # Adjust labels shape for binary classification\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:    # Record loss every 10 mini-batches\n",
    "                sparse_train_losses.append(running_loss / 10)\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Step 7: Evaluate Sparsity and Accuracy after Each Epoch\n",
    "        sparse_model.eval()\n",
    "        prunable_layers = get_prunable_layers(sparse_model)\n",
    "        sparsity = 0.0\n",
    "        total_weights = 0\n",
    "        remaining_weights = 0\n",
    "        for (name, layer) in prunable_layers:\n",
    "            layer_sparsity = tensor_sparsity(layer.weight).item()\n",
    "            sparsity += layer_sparsity\n",
    "            total_weights += layer.weight.numel()\n",
    "            remaining_weights += (layer.weight != 0).sum().item()\n",
    "        sparsity = (remaining_weights / total_weights) * 100 if remaining_weights > 0 else 0\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = sparse_model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                # predicted = (outputs > 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        sparsity_levels.append(sparsity)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f'Epoch {epoch + 1} - Sparsity: {sparsity:.2f}% - Accuracy: {accuracy:.2f}% - Remaining Weights: {remaining_weights}/{total_weights}')\n",
    "        # print(f'Epoch {epoch + 1} - Accuracy: {accuracy:.2f}%')\n",
    "        # print(f'Remaining Weights: {remaining_weights}/{total_weights}')\n",
    "\n",
    "    manager.finalize(sparse_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 60000\n",
      "Number of test images: 10000\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check the number of images in the training and test sets\n",
    "print(f'Number of training images: {len(trainset)}')\n",
    "print(f'Number of test images: {len(testset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu is using for computation\n",
      "\n",
      "Training Base Model:\n",
      "Base Model Accuracy on Test Data: 56.18%\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define the Model Architecture (At least 2 Hidden Layers, 1000 to 5000 weights)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 2)  # 784 -> 2 neurons\n",
    "        self.fc2 = nn.Linear(2, 10)     # 2 -> 10 neurons for classification\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: Train the Base Model\n",
    "model = SimpleNN()\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "num_epochs = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device} is using for computation')\n",
    "\n",
    "train_losses = []\n",
    "print(\"\\nTraining Base Model:\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = inputs.float()  # Ensure inputs are of type float\n",
    "        base_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        base_optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # Record loss every 100 mini-batches\n",
    "            train_losses.append(running_loss / 100)\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "# Step 4: Evaluate the Base Model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.float()  # Ensure images are of type float\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "base_accuracy = 100 * correct / total\n",
    "print(f'Base Model Accuracy on Test Data: {base_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Sparsity Level: 60%\n",
      "\n",
      "Training Sparse Model:\n",
      "Epoch 1 - Sparsity: 100.00% - Accuracy: 56.00% - Remaining Weights: 1588/1588\n",
      "Epoch 2 - Sparsity: 58.63% - Accuracy: 56.27% - Remaining Weights: 931/1588\n",
      "Epoch 3 - Sparsity: 46.66% - Accuracy: 56.02% - Remaining Weights: 741/1588\n",
      "Epoch 4 - Sparsity: 41.37% - Accuracy: 56.29% - Remaining Weights: 657/1588\n",
      "Epoch 5 - Sparsity: 39.99% - Accuracy: 56.34% - Remaining Weights: 635/1588\n",
      "\n",
      "Testing Sparsity Level: 70%\n",
      "\n",
      "Training Sparse Model:\n",
      "Epoch 1 - Sparsity: 100.00% - Accuracy: 56.05% - Remaining Weights: 1588/1588\n",
      "Epoch 2 - Sparsity: 53.27% - Accuracy: 56.17% - Remaining Weights: 846/1588\n",
      "Epoch 3 - Sparsity: 38.29% - Accuracy: 56.14% - Remaining Weights: 608/1588\n",
      "Epoch 4 - Sparsity: 31.74% - Accuracy: 56.14% - Remaining Weights: 504/1588\n",
      "Epoch 5 - Sparsity: 29.97% - Accuracy: 56.20% - Remaining Weights: 476/1588\n",
      "\n",
      "Testing Sparsity Level: 75%\n",
      "\n",
      "Training Sparse Model:\n",
      "Epoch 1 - Sparsity: 100.00% - Accuracy: 56.03% - Remaining Weights: 1588/1588\n",
      "Epoch 2 - Sparsity: 50.57% - Accuracy: 56.40% - Remaining Weights: 803/1588\n",
      "Epoch 3 - Sparsity: 34.13% - Accuracy: 55.99% - Remaining Weights: 542/1588\n",
      "Epoch 4 - Sparsity: 26.89% - Accuracy: 56.28% - Remaining Weights: 427/1588\n",
      "Epoch 5 - Sparsity: 25.00% - Accuracy: 56.27% - Remaining Weights: 397/1588\n",
      "\n",
      "Testing Sparsity Level: 80%\n",
      "\n",
      "Training Sparse Model:\n",
      "Epoch 1 - Sparsity: 100.00% - Accuracy: 56.18% - Remaining Weights: 1588/1588\n",
      "Epoch 2 - Sparsity: 47.92% - Accuracy: 56.26% - Remaining Weights: 761/1588\n",
      "Epoch 3 - Sparsity: 29.97% - Accuracy: 56.13% - Remaining Weights: 476/1588\n",
      "Epoch 4 - Sparsity: 22.04% - Accuracy: 55.99% - Remaining Weights: 350/1588\n",
      "Epoch 5 - Sparsity: 20.03% - Accuracy: 56.28% - Remaining Weights: 318/1588\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Test Various Sparsity Levels\n",
    "sparsity_targets = [0.60, 0.70, 0.75, 0.80]\n",
    "for sparsity_target in sparsity_targets:\n",
    "    print(f\"\\nTesting Sparsity Level: {sparsity_target * 100:.0f}%\")\n",
    "\n",
    "    # Reinitialize the Model for Sparsification\n",
    "    sparse_model = SimpleNN()\n",
    "    sparse_model.load_state_dict(model.state_dict())  # Copy weights from the trained base model\n",
    "    sparse_model = sparse_model.to(device)\n",
    "\n",
    "    # Step 6: Apply Sparsification and Train the Sparse Model\n",
    "    optimizer = torch.optim.SGD(sparse_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "    steps_per_epoch = len(trainloader)\n",
    "\n",
    "    # Create a new sparsification recipe with the target sparsity level\n",
    "    recipe_content = f'''\n",
    "    modifiers:\n",
    "      - !EpochRangeModifier\n",
    "        start_epoch: 0.0\n",
    "        end_epoch: 5.0\n",
    "\n",
    "      - !GlobalMagnitudePruningModifier\n",
    "        params: __ALL_PRUNABLE__\n",
    "        start_epoch: 1.0\n",
    "        end_epoch: 5.0\n",
    "        update_frequency: 0.3\n",
    "        init_sparsity: 0.2\n",
    "        final_sparsity: {sparsity_target}\n",
    "        mask_type: unstructured\n",
    "    '''\n",
    "\n",
    "    with open('temp_recipe.yaml', 'w') as f:\n",
    "        f.write(recipe_content)\n",
    "\n",
    "    manager = ScheduledModifierManager.from_yaml('temp_recipe.yaml')\n",
    "    optimizer = ScheduledOptimizer(optimizer, sparse_model, manager, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "    sparsity_levels = []\n",
    "    accuracies = []\n",
    "    sparse_train_losses = []\n",
    "\n",
    "    print(\"\\nTraining Sparse Model:\")\n",
    "    for epoch in range(num_epochs):\n",
    "        sparse_model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = inputs.float()  # Ensure inputs are of type float\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = sparse_model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:    # Record loss every 100 mini-batches\n",
    "                sparse_train_losses.append(running_loss / 100)\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Step 7: Evaluate Sparsity and Accuracy after Each Epoch\n",
    "        sparse_model.eval()\n",
    "        prunable_layers = get_prunable_layers(sparse_model)\n",
    "        sparsity = 0.0\n",
    "        total_weights = 0\n",
    "        remaining_weights = 0\n",
    "        for (name, layer) in prunable_layers:\n",
    "            layer_sparsity = tensor_sparsity(layer.weight).item()\n",
    "            sparsity += layer_sparsity\n",
    "            total_weights += layer.weight.numel()\n",
    "            remaining_weights += (layer.weight != 0).sum().item()\n",
    "        sparsity = (remaining_weights / total_weights) * 100 if remaining_weights > 0 else 0\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                images = images.float()  # Ensure images are of type float\n",
    "                outputs = sparse_model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        sparsity_levels.append(sparsity)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f'Epoch {epoch + 1} - Sparsity: {sparsity:.2f}% - Accuracy: {accuracy:.2f}% - Remaining Weights: {remaining_weights}/{total_weights}')\n",
    "\n",
    "    manager.finalize(sparse_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
